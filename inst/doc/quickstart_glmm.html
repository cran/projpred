<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Gaussian example</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{projpred Performing variable selection on Generalized Linear Multilevel Models}
\usepackage[utf8](inputenc)
-->

<p>This vignette shows how to use <code>projpred</code> to perform variable selection in the
context of Generalized Linear Multilevel Models (GLMMs). For a general overview
of the package we refer the reader to the quickstart vignette. The method used
here is described in the latest preprint (by Catalina et al. (2020), arxiv
link).</p>

<h2>Gaussian example</h2>

<p>Load the necessary packages. If the sampling takes more than 30 seconds and
multiple cores are available, uncomment the line setting <code>mc.cores</code> to set the
number of cores used (this is commented out as the sampling in the example is
fast and to avoid possible problems when building the vignette along the package
installation in special environments such as computing clusters).</p>

<pre><code class="r">library(projpred)
library(rstanarm)
library(tidyr)
library(dplyr)
library(ggplot2)
library(bayesplot)
theme_set(theme_classic())
#options(mc.cores = 4)
</code></pre>

<p>For the first Gaussian example we borrow the popular <code>orthodont</code> dataset from
the <code>nlme</code> package. This data include observations from a growth curve on an
orthodontic measurement. It contains 108 observations with 4 variables. The
variables contained in this dataset are:</p>

<ul>
<li>distance, numeric distances from the pituitary to the pterygomaxillary 
fissure (mm). These are measured on x-ray images of the skull.</li>
<li>age, the age of the subject (years).</li>
<li>Subject, an ordered factor indicating the subject on which the measurement 
is taken.</li>
<li>Sex, indicator of sex in the subject.</li>
</ul>

<pre><code class="r">data(&quot;Orthodont&quot;, package = &quot;nlme&quot;)
</code></pre>

<p>We first build the following complete <code>rstanarm</code> model including all variables:</p>

<pre><code class="r">fit &lt;- stan_glmer(distance ~ age * Sex + (age | Subject),
                  chains = 2,  data = Orthodont, seed = 1)
</code></pre>

<p>Since we have repeated measurements over subjects at multiple time points, the
effect of <code>age</code> is allowed to vary over subjects. To run <code>projpred</code> on this
model we can first run a simple variable selection with the full dataset. This
approach may be overconfident in some cases and therefore in general cases it is
recommended to run a cross validated variable selection instead.</p>

<pre><code class="r">ref &lt;- get_refmodel(fit)
vs &lt;- varsel(ref)
</code></pre>

<p>Because our model includes group effects, the only available search method is
forward search, which is a bit slower than L_1 search but tends to find smaller
models. Beware that the underlying projection fitting method could return some
warnings in cases where the optimization may not have converged properly. This
is more common in the case of generalized models (non-Gaussian families). We
print the list of the variables ordered by relevance:</p>

<pre><code class="r">solution_terms(vs) # selection order of the variables
</code></pre>

<p>We plot some statistics computed on the training data, such as the sum of log
predictive densities (ELPD) and root mean squared error (RMSE) as the function
of number of variables added. By default, the statistics are shown on absolute
scale, but with <code>deltas = TRUE</code> the plot shows results relative to the full
model.</p>

<pre><code class="r"># plot predictive performance on training data
plot(vs, stats = c(&#39;elpd&#39;, &#39;rmse&#39;))
</code></pre>

<p>From this plot, it is clearly visible that the first two terms are probaly
enough to achieve predictive performance comparable to the reference model. We
perform the projection for a submodel of desired size using the function
<code>project</code>. The projection can also be coerced to a matrix with draws of the
selected variables and sigma. The draws can be visualized with, for example, the
<code>mcmc_areas</code> function in the <code>bayesplot</code> package. Below we compare how the
projection affects the three most relevant variables.</p>

<!-- ```{r, fig.width=6, fig.height=2} -->

<!--  # Visualise the three most relevant variables in the full model -\-> &ndash;>

<!--  mcmc_areas(as.matrix(vs$refmodel$fit), -->

<!--             pars = c("(Intercept)", "Subject__Intercept", "age", -->

<!--                      "sd_Subject__age", "sigma")) -->

<!-- ``` -->

<pre><code class="r"># Visualise the projected three most relevant variables
proj &lt;- project(vs, nterms = 2, ns = 500)
mcmc_areas(as.matrix(proj), pars = solution_terms(vs)[1:2])
</code></pre>

<p>We make predictions with the projected submodels. For point estimates we can use
method <code>proj_linpred</code>. Test inputs can be provided using the keyword <code>newdata</code>.
If also the test targets <code>ynew</code> are provided, then the function evaluates the
log predictive density at these points. For instance, the following computes the
mean of the predictive distribution and evaluates the log density at the
training points using the 5 most relevant variables.</p>

<pre><code class="r">pred &lt;- proj_linpred(vs, newdata = Orthodont, nterms = 5, integrated = TRUE)
</code></pre>

<p>Visualize the predictions</p>

<pre><code class="r">ggplot() +
  geom_point(aes(x = pred$pred, y = Orthodont$distance)) +
  geom_abline(slope = 1, color = &quot;red&quot;) +
  labs(x = &quot;prediction&quot;, y = &quot;y&quot;)
</code></pre>

<p>We also obtain draws from the projected predictive distribution. Here&#39;s an
example prediction for the first data point using 5 terms (the observed
value is marked by the red line).</p>

<pre><code class="r">subset &lt;- Orthodont %&gt;% as_tibble() %&gt;% dplyr::sample_n(1)
y_subset &lt;- subset %&gt;% dplyr::select(distance) %&gt;% as.data.frame()
y1_rep &lt;- proj_predict(vs, newdata = subset, nterms = 5, seed = 7560)
qplot(as.vector(y1_rep), bins = 25) +
  geom_vline(xintercept = as.numeric(y_subset), color = &quot;red&quot;) +
  xlab(&quot;y1_rep&quot;)
</code></pre>

<h2>Poisson example</h2>

<p>As previously, we first load the data,</p>

<pre><code class="r">data_pois &lt;- read.table(&quot;data_pois.csv&quot;, header = TRUE)
</code></pre>

<p>These data correspond to a phylogenetic dataset where the phenotype is expressed
as counts. This kind of data is relevant in evolutionary biology when data of
many species are analyzed at the same time. The model we fit here is borrowed
from <em>Modern Phylogenetic Comparative Methods and the application in
Evolutionary Biology</em> (de Villemeruil &amp; Nakagawa, 2014). The necessary data can
also be downloaded from the corresponding website
(<a href="http://www.mpcm-evolution.com/">http://www.mpcm-evolution.com/</a>). The specific formula is taken from the
<em>Estimating Phylogenetic Multilevel Models with brms</em> vignette of the brms
package
(<a href="https://cran.r-project.org/package=brms/vignettes/brms_phylogenetics.html">https://cran.r-project.org/package=brms/vignettes/brms_phylogenetics.html</a>).</p>

<pre><code class="r">fit &lt;- stan_glmer(
  phen_pois ~ cofactor + (1 | phylo) + (1 | obs), data = data_pois,
  family = poisson(&quot;log&quot;), chains = 2, iter = 2000,
  control = list(adapt_delta = 0.95)
)
</code></pre>

<p>As we did in the previous example, we can perform variable selection and look
at the projection.</p>

<pre><code class="r">vs &lt;- varsel(fit)
</code></pre>

<p>Because our model includes group effects, the only available search method is
forward search, which is a bit slower than L_1 search but tends to find smaller
models. Beware that the underlying projection fitting method could return some
warnings in cases where the optimization may not have converged properly. This
is more common in the case of generalized models (non-Gaussian families). We
print the list of the variables ordered by relevance:</p>

<pre><code class="r">solution_terms(vs) # selection order of the variables
</code></pre>

<p>We plot some statistics computed on the training data, such as the sum of log
predictive densities (ELPD) and root mean squared error (RMSE) as the function
of number of variables added. By default, the statistics are shown on absolute
scale, but with <code>deltas = TRUE</code> the plot shows results relative to the full
model.</p>

<pre><code class="r"># plot predictive performance on training data
plot(vs, stats = c(&#39;elpd&#39;, &#39;rmse&#39;))
</code></pre>

<p>We perform the projection for a submodel of desired size using the function
<code>project</code>. The projection can also be coerced to a matrix with draws of the
selected variables and sigma. The draws can be visualized with, for example, the
<code>mcmc_areas</code> function in the <code>bayesplot</code> package. Below we compare how the
projection affects the most relevant variables.</p>

<!-- ```{r, fig.width=6, fig.height=2} -->

<!--  # Visualise the most relevant variable in the full model -\-> &ndash;>

<!--  mcmc_areas(as.matrix(vs$refmodel$fit), -->

<!--             pars = c("b_Intercept", "b_cofactor", "sd_phylo__Intercept")) -->

<!-- ``` -->

<pre><code class="r"># Visualise the projected two most relevant variables
proj &lt;- project(vs, nterms = 2, ndraws = 10)
mcmc_areas(as.matrix(proj), pars = solution_terms(vs)[1:2])
</code></pre>

<p>As we did in the Gaussian example, we make predictions with the projected
submodels. The following computes the mean of the predictive distribution and
evaluates the log density at the training points using the previously projected
model.</p>

<pre><code class="r">pred &lt;- proj_linpred(proj, newdata = data_pois, integrated = TRUE)
</code></pre>

<p>We can also visualize the corresponding projected predictions.</p>

<pre><code class="r">xaxis &lt;- seq(-3, 3, length.out = 1000)
y_mu &lt;- rowMeans(vs$refmodel$mu)
ggplot() +
  geom_point(aes(x = pred$pred, y = y_mu)) +
  geom_line(aes(x=xaxis, y = exp(xaxis)), color = &quot;red&quot;) +
  labs(x = &quot;prediction&quot;, y = &quot;y&quot;)
</code></pre>

<h2>Bernoulli example</h2>

<p>For both of the previous cases, running variable selection with the full data
was actually enough because they were pretty simple. In this section we work on
a slightly more difficult data set.</p>

<p>We can load the data from the popular <code>lme4</code> package:</p>

<pre><code class="r">data(&quot;VerbAgg&quot;, package = &quot;lme4&quot;)
</code></pre>

<p>The whole dataset consists of 7584 questionnaire answers from different
subjects. Given that the full data could take a long time to fit, we will
subsample 50 individuals (for which sampling still takes a bit). For this, we
use the great <code>tidyverse</code> environment.</p>

<pre><code class="r">## subsample 50 participants
VerbAgg_subsample &lt;- VerbAgg %&gt;%
  tidyr::as_tibble() %&gt;%
  dplyr::filter(id %in% sample(id, 50)) %&gt;%
  dplyr::mutate(r2num = as.integer(r2) - 1) # binomial family needs numeric target
</code></pre>

<p>For this simple model we will add some group effects that we know are not quite
relevant.</p>

<pre><code class="r">## simple bernoulli model
formula_va &lt;- r2num ~ btype + situ + mode + (btype + situ + mode | id)
fit_va &lt;- stan_glmer(
  formula = formula_va,
  data = VerbAgg_subsample,
  family = binomial(&quot;logit&quot;),
  seed = 1234,
  chains = 2
)
</code></pre>

<p>As we did before, we can run the standard variable selection with</p>

<pre><code class="r">vs_va &lt;- varsel(fit_va)
</code></pre>

<pre><code class="r">solution_terms(vs_va)
</code></pre>

<pre><code class="r">plot(vs_va, stats = c(&quot;elpd&quot;, &quot;acc&quot;))
</code></pre>

<p>Even though the ordering of the variables makes sense, the performance of the
projected models does not quite match the reference model. There may be
different reasons that can explain this behaviour:</p>

<ol>
<li>The reference model posterior may not be very narrow and then running
<code>varsel</code> with the default <code>ndraws_pred</code> could be not enough. Increasing
<code>ndraws_pred</code> helps but also increases the computational cost. Re fitting the
reference model ensuring a narrower posterior (usually employing a stronger
sparsifying prior) would have a similar effect. This is usually the case when
the difference in predictive performance is not very large.</li>
<li>For non-Gaussian models the source of the inaccuracy may come from the fact
that we are running an approximate projection. In this case, increasing the
number of draws for the prediction <code>ndraws_pred</code> should also help.</li>
<li>Given that <code>varsel</code> computes the expected predictive performance of the
reference model from the in-sample mean, it may sometimes result in
overconfident results. Running <code>cv_varsel</code> with LOO cross-validation computes
the predictive performance by taking PSIS-LOO weights into account, usually
reporting a more realistic ELPD.</li>
</ol>

<p>By default, <code>cv_varsel</code> will run LOO cross-validation, and will try to run a
full variable selection for every data point. Given the large computational cost
of running more draws, we&#39;ll first try running <code>cv_varsel</code> without validating
the search. We can omit this behaviour by fitting the model once and computing
the LOO elpd posterior from it by passing the argument <code>validate_search =
FALSE</code>. Note that in general it is recommended to run the full validation
search.</p>

<pre><code class="r">cv_vs_va &lt;- cv_varsel(fit_va, validate_search = FALSE)
</code></pre>

<pre><code class="r">plot(cv_vs_va, stats = c(&quot;elpd&quot;, &quot;acc&quot;))
</code></pre>

<p>Now we see that the projected models behave as expected and can proceed with the
usual analysis.</p>

<p>As in previous examples, we can show the mean prediction with <code>proj_linpred</code>.
The following computes the mean of the predictive distribution and evaluates the
log density at the training points using the 6 most relevant variables.</p>

<pre><code class="r">pred &lt;- proj_linpred(cv_vs_va, newdata = VerbAgg_subsample,
                     nterms = 6, integrated = TRUE, ndraws = 10)
</code></pre>

<p>We can also visualize the corresponding projected predictions.</p>

<pre><code class="r">xaxis &lt;- seq(-6, 6, length.out = 1000)
yaxis &lt;- cv_vs_va$family$linkinv(xaxis)

y_mu &lt;- rowMeans(cv_vs_va$refmodel$mu)
ggplot() +
  geom_point(aes(x = pred$pred, y = y_mu)) +
  geom_line(aes(x = xaxis, y = yaxis), color = &quot;red&quot;) +
  labs(x = &quot;prediction&quot;, y = &quot;y&quot;)
</code></pre>

</body>

</html>
