<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />


<meta name="date" content="2022-04-02" />

<title>projpred: Projection predictive feature selection</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">projpred: Projection predictive feature
selection</h1>
<h4 class="date">2022-04-02</h4>


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#data">Data</a></li>
<li><a href="#reference-model">Reference model</a></li>
<li><a href="#variable-selection">Variable selection</a></li>
<li><a href="#post-selection-inference">Post-selection inference</a>
<ul>
<li><a href="#marginals-of-the-projected-posterior">Marginals of the
projected posterior</a></li>
<li><a href="#predictions">Predictions</a></li>
</ul></li>
<li><a href="#refmodtypes">Supported types of reference models</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p><strong>Note:</strong> The output of this vignette is shown <a href="https://mc-stan.org/projpred/articles/projpred.html">here</a>.</p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This vignette shows the main functionalities of the
<strong>projpred</strong> package, which implements the projective
variable selection for generalized linear and additive models as well as
for generalized linear and additive multilevel models.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> What is special about
the projective variable selection is that it not only performs a
variable selection, but also allows for valid post-selection
inference.</p>
<p>The projective variable selection is based on the ideas of Goutis and
Robert (1998) and Dupuis and Robert (2003). The methods implemented in
<strong>projpred</strong> are described in detail in Piironen et
al.Â (2020) and Catalina et al.Â (2020). They are evaluated in comparison
to many other methods in Piironen and Vehtari (2017a). Type
<code>citation(&quot;projpred&quot;)</code> for details on how to cite
<strong>projpred</strong>.</p>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>For this vignette, we use <strong>projpred</strong>âs
<code>df_gaussian</code> data. It contains 100 observations of 20
continuous predictor variables <code>X1</code>, â¦, <code>X20</code>
(originally stored in a sub-matrix; we turn them into separate columns
below) and one continuous response variable <code>y</code>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;df_gaussian&quot;</span>, <span class="at">package =</span> <span class="st">&quot;projpred&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>dat_gauss <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> df_gaussian<span class="sc">$</span>y, df_gaussian<span class="sc">$</span>x)</span></code></pre></div>
</div>
<div id="reference-model" class="section level2">
<h2>Reference model</h2>
<p>First, we have to construct a reference model for the projective
variable selection. This model is considered as the best (âreferenceâ)
solution to the prediction task. The aim of the projective variable
selection is to find a subset of a set of candidate predictors which is
as small as possible but achieves a predictive performance as close as
possible to that of the reference model.</p>
<p>The <strong>projpred</strong> package is compatible with reference
models fit by the <strong>rstanarm</strong> and <strong>brms</strong>
packages. To our knowledge, <strong>rstanarm</strong> and
<strong>brms</strong> are currently the only packages for which a
<code>get_refmodel()</code> method (which establishes the compatibility
with <strong>projpred</strong>) exists. Custom reference models can be
constructed via <code>init_refmodel()</code>, as shown in section
âExamplesâ in the <code>?init_refmodel</code> help.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> For both,
<strong>rstanarm</strong> and <strong>brms</strong> reference models,
all candidate models are <em>submodels</em> of the reference model. In
principle, this is not a necessary assumption for a projective variable
selection (see, e.g., Piironen et al., 2020) and custom reference models
allow to avoid this assumption, but for <strong>rstanarm</strong> and
<strong>brms</strong> reference models, this is a reasonable assumption
which simplifies implementation in <strong>projpred</strong> a lot.</p>
<p>Here, we use the <strong>rstanarm</strong> package to fit the
reference model. If you want to use the <strong>brms</strong> package,
you simply have to replace the <strong>rstanarm</strong> fit (of class
<code>stanreg</code>) in all the code below by your
<strong>brms</strong> fit (of class <code>brmsfit</code>). Only note
that in case of a <strong>brms</strong> fit, argument
<code>brms_seed</code> of <code>brms::get_refmodel.brmsfit()</code> is
necessary to ensure reproducible results in a K-fold CV.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rstanarm)</span></code></pre></div>
<p>For our <strong>rstanarm</strong> reference model, we use the
Gaussian distribution as the <code>family</code> for our response. With
respect to the predictors, we only include the linear main effects of
all 20 predictor variables. Compared to the more complex types of
reference models supported by <strong>projpred</strong> (see section <a href="#refmodtypes">âSupported types of reference modelsâ</a> below),
this is a quite simple reference model which is sufficient, however, to
demonstrate the interplay of <strong>projpred</strong>âs functions.</p>
<p>We use <strong>rstanarm</strong>âs default priors in our reference
model, except for the regression coefficients for which we use a
regularized horseshoe prior (Piironen and Vehtari, 2017c) with the
hyperprior for its global shrinkage parameter following Piironen and
Vehtari (2017b,c). In R code, these are the preparation steps for the
regularized horseshoe prior:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of regression coefficients:</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>( D <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">grepl</span>(<span class="st">&quot;^X&quot;</span>, <span class="fu">names</span>(dat_gauss))) )</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior guess for the number of relevant (i.e., non-zero) regression</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># coefficients:</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>p0 <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of observations:</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">nrow</span>(dat_gauss)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperprior scale for tau, the global shrinkage parameter (note that for the</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Gaussian family, &#39;rstanarm&#39; will automatically scale this by the residual</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># standard deviation):</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>tau0 <span class="ot">&lt;-</span> p0 <span class="sc">/</span> (D <span class="sc">-</span> p0) <span class="sc">*</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(N)</span></code></pre></div>
<p>We now fit the reference model to the data. To make this vignette
build faster, we use only 2 MCMC chains and 500 iterations per chain
(with half of them being discarded as warmup draws). In practice, 4
chains and 2000 iterations per chain are reasonable defaults.
Furthermore, we make use of <strong>rstan</strong>âs parallelization,
which means to run each chain on a separate CPU core.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> If you run the
following code yourself, you can either rely on an automatic mechanism
to detect the number of CPU cores (like the
<code>parallel::detectCores()</code> function shown below) or adapt
<code>ncores</code> manually to your system.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set this manually if desired:</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>ncores <span class="ot">&lt;-</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>(<span class="at">logical =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="do">### Only for technical reasons in this vignette (you can omit this when running</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="do">### the code yourself):</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>ncores <span class="ot">&lt;-</span> <span class="fu">min</span>(ncores, 2L)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="do">###</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">mc.cores =</span> ncores)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>refm_fit <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  y <span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3 <span class="sc">+</span> X4 <span class="sc">+</span> X5 <span class="sc">+</span> X6 <span class="sc">+</span> X7 <span class="sc">+</span> X8 <span class="sc">+</span> X9 <span class="sc">+</span> X10 <span class="sc">+</span> X11 <span class="sc">+</span> X12 <span class="sc">+</span> X13 <span class="sc">+</span> X14 <span class="sc">+</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    X15 <span class="sc">+</span> X16 <span class="sc">+</span> X17 <span class="sc">+</span> X18 <span class="sc">+</span> X19 <span class="sc">+</span> X20,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">gaussian</span>(),</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> dat_gauss,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">prior =</span> <span class="fu">hs</span>(<span class="at">global_scale =</span> tau0),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>  <span class="do">### Only for the sake of speed (not recommended in general):</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">chains =</span> <span class="dv">2</span>, <span class="at">iter =</span> <span class="dv">500</span>,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  <span class="do">###</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">seed =</span> <span class="dv">2052109</span>, <span class="at">QR =</span> <span class="cn">TRUE</span>, <span class="at">refresh =</span> <span class="dv">0</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Usually, we would now have to check the convergence diagnostics (see,
e.g., <code>?posterior::diagnostics</code> and
<code>?posterior::default_convergence_measures</code>). However, due to
the technical reasons for which we reduced <code>chains</code> and
<code>iter</code>, we skip this step here.</p>
</div>
<div id="variable-selection" class="section level2">
<h2>Variable selection</h2>
<p>Now, <strong>projpred</strong> comes into play.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(projpred)</span></code></pre></div>
<p>In <strong>projpred</strong>, the projective variable selection
consists of a <em>search</em> part and an <em>evaluation</em> part. The
search part determines the solution path, i.e., the best submodel for
each submodel size (number of predictor terms). The evaluation part
determines the predictive performance of the submodels along the
solution path.</p>
<p>There are two functions for performing the variable selection:
<code>varsel()</code> and <code>cv_varsel()</code>. In contrast to
<code>varsel()</code>, <code>cv_varsel()</code> performs a
cross-validation (CV) by running the search part with the training data
of each CV fold separately (an exception is
<code>validate_search = FALSE</code>, see <code>?cv_varsel</code>) and
running the evaluation part on the corresponding test set of each CV
fold. Because of this CV, <code>cv_varsel()</code> is recommended over
<code>varsel()</code>. Thus, we use <code>cv_varsel()</code> here.
Nonetheless, running <code>varsel()</code> first can offer a rough idea
of the performance of the projections (i.e., of the submodels after
projecting the reference model onto them). The same holds for
<code>cv_varsel()</code> with <code>validate_search = FALSE</code>
(which only takes effect for <code>cv_method = &quot;LOO&quot;</code>). A more
principled <strong>projpred</strong> workflow is work under
progress.</p>
<!-- In versions > 2.0.2, **projpred** offers a parallelization of the projection. Typically, this only makes sense for a large number of projected draws. Therefore, this parallelization is not activated by a simple logical switch, but by a threshold for the number of projected draws below which no parallelization will be used. Values greater than or equal to this threshold will trigger the parallelization. For more information, see the general package documentation available at ``?`projpred-package` ``. There, we also explain why we are not running the parallelization on Windows and why we cannot recommend the parallelization of the projection for some types of reference models (see also section ["Supported types of reference models"](#refmodtypes) below). -->
<!-- ```{r} -->
<!-- if (!identical(.Platform$OS.type, "windows")) { -->
<!--   trigger_default <- options(projpred.prll_prj_trigger = 200) -->
<!--   library(doParallel) -->
<!--   registerDoParallel(ncores) -->
<!-- } -->
<!-- ``` -->
<p>Here, we use only some of the available arguments; see the
documentation of <code>cv_varsel()</code> for the full list of
arguments. By modifying argument <code>cv_method</code>, we use a K-fold
CV instead of a leave-one-out (LOO) CV. We do this here only to make
this vignette build faster. When the computation time is not an issue,
we recommend using LOO-CV as it is likely to be more accurate.
Similarly, we set <code>nclusters_pred</code> to a low value of
<code>20</code> only to speed up the building of the vignette. By
modifying argument <code>nterms_max</code>, we impose a limit on the
submodel size until which the search is continued. Typically, one has to
run the variable selection with a large <code>nterms_max</code> first
(the default value may not even be large enough) and only after
inspecting the results from this first run, one is able to set a
reasonable <code>nterms_max</code> in subsequent runs. The value we are
using here (<code>9</code>) is based on such a first run (which is not
shown here, though).</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>cvvs <span class="ot">&lt;-</span> <span class="fu">cv_varsel</span>(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  refm_fit,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="do">### Only for the sake of speed (not recommended in general):</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">cv_method =</span> <span class="st">&quot;kfold&quot;</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">nclusters_pred =</span> <span class="dv">20</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="do">###</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">nterms_max =</span> <span class="dv">9</span>,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">seed =</span> <span class="dv">411183</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>The first step after running the variable selection should be the
decision for a final submodel size. This should be the first step (in
particular, before inspecting the solution path) in order to avoid a
user-induced selection bias (which could occur if the user made the
submodel size decision dependent on the solution path). To decide for a
submodel size, there are several performance statistics we can plot as a
function of the submodel size. Here, we use the expected log (pointwise)
predictive density (for a new dataset) (ELPD; empirically, this is the
sum of the log predictive densities of the observations in the
evaluationâor âtestââset) and the root mean squared error (RMSE). By
default, the performance statistics are plotted on their original scale,
but with <code>deltas = TRUE</code>, they are calculated as differences
from a baseline model (which is the reference model by default, at least
in the most common cases). Since the differences are usually of more
interest (with regard to the submodel size decision), we directly plot
with <code>deltas = TRUE</code> here:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cvvs, <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">&quot;elpd&quot;</span>, <span class="st">&quot;rmse&quot;</span>), <span class="at">deltas =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Based on that plot, we would decide for a submodel size of 6 because
thatâs the point where the performance measures level off and are close
enough to the reference modelâs performance.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>modsize_decided <span class="ot">&lt;-</span> <span class="dv">6</span></span></code></pre></div>
<p>Note that <strong>projpred</strong> offers the
<code>suggest_size()</code> function which may help in the decision for
a submodel size, but this is a rather heuristic method and needs to be
interpreted with caution (see <code>?suggest_size</code>).</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suggest_size</span>(cvvs)</span></code></pre></div>
<p>Here, we would get the same final submodel size (<code>6</code>) as
by our manual decision.</p>
<p>Only now, after we have made a decision for the submodel size, we
inspect further results from the variable selection and, in particular,
the solution path. For example, we can simply <code>print()</code> the
resulting object:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>cvvs</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="do">### Alternative modifying the number of printed decimal places:</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print(cvvs, digits = 2)</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="do">### </span></span></code></pre></div>
<p>The solution path can be seen in the <code>print()</code> output
(column <code>solution_terms</code>), but it is also accessible through
the <code>solution_terms()</code> function:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>( soltrms <span class="ot">&lt;-</span> <span class="fu">solution_terms</span>(cvvs) )</span></code></pre></div>
<p>Combining the decided submodel size of 6 with the solution path leads
to the following terms (as well as the intercept) as the predictor terms
of the final submodel:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>( soltrms_final <span class="ot">&lt;-</span> <span class="fu">head</span>(soltrms, modsize_decided) )</span></code></pre></div>
</div>
<div id="post-selection-inference" class="section level2">
<h2>Post-selection inference</h2>
<p>The <code>project()</code> function returns an object of class
<code>projection</code> which forms the basis for convenient
post-selection inference. By the following code, <code>project()</code>
will project the reference model onto the final submodel once again<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>prj <span class="ot">&lt;-</span> <span class="fu">project</span>(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  refm_fit,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">solution_terms =</span> soltrms_final,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">seed =</span> <span class="dv">15705533</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<!-- Alternative, as soon as GitHub issue #168 is resolved: -->
<!-- ```{r} -->
<!-- prj <- project( -->
<!--   cvvs, -->
<!--   nterms = modsize_decided, -->
<!--   refit_prj = FALSE, -->
<!--   seed = 15705533 -->
<!-- ) -->
<!-- ``` -->
<p>For more accurate results, we could have increased argument
<code>ndraws</code> of <code>project()</code> (up to the number of
posterior draws in the reference model). This increases the runtime,
which we donât want in this vignette.</p>
<p>Next, we create a matrix containing the projected posterior draws
stored in the depths of <code>project()</code>âs output:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>prj_mat <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(prj)</span></code></pre></div>
<p>This matrix is all we need for post-selection inference. It can be
used like any matrix of draws from MCMC procedures, except that it
doesnât reflect a typical posterior distribution, but rather a projected
posterior distribution, i.e., the distribution arising from the
deterministic projection of the reference modelâs posterior distribution
onto the parameter space of the final submodel.</p>
<div id="marginals-of-the-projected-posterior" class="section level3">
<h3>Marginals of the projected posterior</h3>
<p>The <strong>posterior</strong> package provides a general way to deal
with posterior distributions, so it can also be applied to our projected
posterior. For example, to calculate summary statistics for the
marginals of the projected posterior:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(posterior)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>prj_drws <span class="ot">&lt;-</span> <span class="fu">as_draws_matrix</span>(prj_mat)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># In the following call, as.data.frame() is used only because pkgdown</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># versions &gt; 1.6.1 don&#39;t print the tibble correctly.</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.frame</span>(<span class="fu">summarize_draws</span>(</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  prj_drws,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;median&quot;</span>, <span class="st">&quot;mad&quot;</span>, <span class="cf">function</span>(x) <span class="fu">quantile</span>(x, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>))</span></code></pre></div>
<p>A visualization of the projected posterior can be achieved with the
<strong>bayesplot</strong> package, for example using its
<code>mcmc_intervals()</code> function.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bayesplot)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">bayesplot_theme_set</span>(ggplot2<span class="sc">::</span><span class="fu">theme_bw</span>())</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mcmc_intervals</span>(prj_mat) <span class="sc">+</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  ggplot2<span class="sc">::</span><span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.5</span>, <span class="fl">1.6</span>))</span></code></pre></div>
<p>Note that we only visualize the <em>1-dimensional</em> marginals of
the projected posterior here. To gain a more complete picture, we would
have to visualize at least some <em>2-dimensional</em> marginals of the
projected posterior (i.e., marginals for pairs of parameters).</p>
<p>For comparison, consider the marginal posteriors of the corresponding
parameters in the reference model:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>refm_mat <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(refm_fit)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mcmc_intervals</span>(refm_mat, <span class="at">pars =</span> <span class="fu">colnames</span>(prj_mat)) <span class="sc">+</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  ggplot2<span class="sc">::</span><span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.5</span>, <span class="fl">1.6</span>))</span></code></pre></div>
<p>Here, the reference modelâs marginal posteriors differ only slightly
from the marginals of the projected posterior. This does not necessarily
have to be the case.</p>
</div>
<div id="predictions" class="section level3">
<h3>Predictions</h3>
<p>Predictions from the final submodel can be made by
<code>proj_linpred()</code> and <code>proj_predict()</code>.</p>
<p>We start with <code>proj_linpred()</code>. For example, suppose we
have the following new observations:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>( dat_gauss_new <span class="ot">&lt;-</span> <span class="fu">setNames</span>(</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>(<span class="fu">replicate</span>(<span class="fu">length</span>(soltrms_final), <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>))),</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  soltrms_final</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>) )</span></code></pre></div>
<p>Then <code>proj_linpred()</code> can calculate the linear
predictors<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> for all new observations from
<code>dat_gauss_new</code>. Depending on argument
<code>integrated</code>, these linear predictors can be averaged across
the projected draws (within each new observation). For instance, the
following computes the expected values of the new observationsâ
predictive distributions:<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>prj_linpred <span class="ot">&lt;-</span> <span class="fu">proj_linpred</span>(prj, <span class="at">newdata =</span> dat_gauss_new, <span class="at">integrated =</span> <span class="cn">TRUE</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(dat_gauss_new, <span class="at">linpred =</span> <span class="fu">as.vector</span>(prj_linpred<span class="sc">$</span>pred))</span></code></pre></div>
<p>If <code>dat_gauss_new</code> also contained response values (i.e.,
<code>y</code> values in this example), then <code>proj_linpred()</code>
would also evaluate the log predictive density at these.</p>
<p>With <code>proj_predict()</code>, we can obtain draws from predictive
distributions based on the final submodel. In contrast to
<code>proj_linpred(&lt;...&gt;, integrated = FALSE)</code>, this
encompasses not only the uncertainty arising from parameter estimation,
but also the uncertainty arising from the observational (or âsamplingâ)
model for the response.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> This is useful for what is usually termed a
posterior predictive check (PPC), but would have to be termed something
like a posterior-projected predictive check (PPPC) here.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>prj_predict <span class="ot">&lt;-</span> <span class="fu">proj_predict</span>(prj, <span class="at">.seed =</span> <span class="dv">762805</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Using the &#39;bayesplot&#39; package:</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ppc_dens_overlay</span>(<span class="at">y =</span> dat_gauss<span class="sc">$</span>y, <span class="at">yrep =</span> prj_predict, <span class="at">alpha =</span> <span class="fl">0.9</span>, <span class="at">bw =</span> <span class="st">&quot;SJ&quot;</span>)</span></code></pre></div>
<p>This PPPC shows that our final projection is able to generate
predictions similar to the observed response values, which indicates
that this model is reasonable, at least in this regard.</p>
<!-- ## Teardown / clean-up -->
<!-- Finally, we clean up everything we have set up for the parallelization of the projection. This may not always be necessary, but sometimes it is and apart from that, it is simply good practice: -->
<!-- ```{r} -->
<!-- if (!identical(.Platform$OS.type, "windows")) { -->
<!--   stopImplicitCluster() -->
<!--   registerDoSEQ() -->
<!--   options(trigger_default) -->
<!-- } -->
<!-- ``` -->
</div>
</div>
<div id="refmodtypes" class="section level2">
<h2>Supported types of reference models</h2>
<p>Apart from the <code>gaussian()</code> response family used in this
vignette, <strong>projpred</strong> also supports the
<code>binomial()</code><a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> and the <code>poisson()</code> family. On
the side of the predictors, <strong>projpred</strong> not only supports
linear main effects as shown in this vignette, but also interactions,
multilevel<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>, andâas an experimental featureâadditive<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>
terms.</p>
<p>Transferring this vignette to such more complex reference models is
straightforward: Basically, only the code for fitting the reference
model via <strong>rstanarm</strong> or <strong>brms</strong> needs to be
adapted. The <strong>projpred</strong> code stays almost the same. Only
note that in case of multilevel or additive reference models,
<!-- the parallelization of the projection is not recommended and that -->
some <strong>projpred</strong> functions then have slightly different
options for a few arguments. See the documentation for details.</p>
<p>For example, to apply <strong>projpred</strong> to the
<code>VerbAgg</code> dataset from the <strong>lme4</strong> package, a
corresponding multilevel reference model for the binary response
<code>r2</code> could be created by the following code:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;VerbAgg&quot;</span>, <span class="at">package =</span> <span class="st">&quot;lme4&quot;</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>refm_fit <span class="ot">&lt;-</span> <span class="fu">stan_glmer</span>(</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  r2 <span class="sc">~</span> btype <span class="sc">+</span> situ <span class="sc">+</span> mode <span class="sc">+</span> (btype <span class="sc">+</span> situ <span class="sc">+</span> mode <span class="sc">|</span> id),</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">binomial</span>(),</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> VerbAgg,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">seed =</span> <span class="dv">82616169</span>, <span class="at">QR =</span> <span class="cn">TRUE</span>, <span class="at">refresh =</span> <span class="dv">0</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>As an example for an additive (non-multilevel) reference model,
consider the <code>lasrosas.corn</code> dataset from the
<strong>agridat</strong> package. A corresponding reference model for
the continuous response <code>yield</code> could be created by the
following code (note that <code>pp_check(refm_fit)</code> gives a bad
PPC in this case, so thereâs still room for improvement):</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;lasrosas.corn&quot;</span>, <span class="at">package =</span> <span class="st">&quot;agridat&quot;</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert `year` to a `factor` (this could also be solved by using</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `factor(year)` in the formula, but we avoid that here to put more emphasis on</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the demonstration of the smooth term):</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>lasrosas.corn<span class="sc">$</span>year <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(lasrosas.corn<span class="sc">$</span>year)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>refm_fit <span class="ot">&lt;-</span> <span class="fu">stan_gamm4</span>(</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>  yield <span class="sc">~</span> year <span class="sc">+</span> topo <span class="sc">+</span> <span class="fu">t2</span>(nitro, bv),</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">gaussian</span>(),</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> lasrosas.corn,</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">seed =</span> <span class="dv">4919670</span>, <span class="at">QR =</span> <span class="cn">TRUE</span>, <span class="at">refresh =</span> <span class="dv">0</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>As an example for an additive multilevel reference model, consider
the <code>gumpertz.pepper</code> dataset from the
<strong>agridat</strong> package. A corresponding reference model for
the binary response <code>disease</code> could be created by the
following code:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;gumpertz.pepper&quot;</span>, <span class="at">package =</span> <span class="st">&quot;agridat&quot;</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>refm_fit <span class="ot">&lt;-</span> <span class="fu">stan_gamm4</span>(</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  disease <span class="sc">~</span> field <span class="sc">+</span> leaf <span class="sc">+</span> <span class="fu">s</span>(water),</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">random =</span> <span class="sc">~</span> (<span class="dv">1</span> <span class="sc">|</span> row) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> quadrat),</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">binomial</span>(),</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> gumpertz.pepper,</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">seed =</span> <span class="dv">14209013</span>, <span class="at">QR =</span> <span class="cn">TRUE</span>, <span class="at">refresh =</span> <span class="dv">0</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div id="troubleshooting" class="section level2">
<h2>Troubleshooting</h2>
<p>Sometimes, the ordering of the predictor terms in the solution path
makes sense, but for increasing submodel size, the performance measures
of the projections do not approach that of the reference model. There
are different reasons that can explain this behavior (the following list
might not be exhaustive, though):</p>
<ol style="list-style-type: decimal">
<li>The reference modelâs posterior may be so wide that the default
<code>ndraws_pred</code> could be too small. Usually, this comes in
combination with a difference in predictive performance which is
comparatively small. Increasing <code>ndraws_pred</code> should help,
but it also increases the computational cost. Re-fitting the reference
model and thereby ensuring a narrower posterior (usually by employing a
stronger sparsifying prior) should have a similar effect.</li>
<li>For non-Gaussian models, the discrepancy may be due to the fact that
the penalized iteratively reweighted least squares (PIRLS) algorithm
might have convergence issues (Catalina et al., 2021). In this case, the
latent-space approach by Catalina et al.Â (2021) might help.</li>
<li>If you are using <code>varsel()</code>, then the lack of the CV in
<code>varsel()</code> may lead to overconfident and overfitted results.
In this case, try running <code>cv_varsel()</code> instead of
<code>varsel()</code> (which you should in any case for your final
results).</li>
</ol>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Catalina, A., BÃ¼rkner, P.-C., and Vehtari, A. (2020). Projection
predictive inference for generalized linear and additive multilevel
models. <em>arXiv:2010.06994</em>. URL: <a href="https://arxiv.org/abs/2010.06994" class="uri">https://arxiv.org/abs/2010.06994</a>.</p>
<p>Catalina, A., BÃ¼rkner, P., and Vehtari, A. (2021). Latent space
projection predictive inference. <em>arXiv:2109.04702</em>. URL: <a href="https://arxiv.org/abs/2109.04702" class="uri">https://arxiv.org/abs/2109.04702</a>.</p>
<p>Dupuis, J. A. and Robert, C. P. (2003). Variable selection in
qualitative models via an entropic explanatory power. <em>Journal of
Statistical Planning and Inference</em>,
<strong>111</strong>(1-2):77â94. DOI: <a href="https://doi.org/10.1016/S0378-3758(02)00286-0">10.1016/S0378-3758(02)00286-0</a>.</p>
<p>Goutis, C. and Robert, C. P. (1998). Model choice in generalised
linear models: A Bayesian approach via KullbackâLeibler projections.
<em>Biometrika</em>, <strong>85</strong>(1):29â37.</p>
<p>Piironen, J., Paasiniemi, M., and Vehtari, A. (2020). Projective
inference in high-dimensional problems: Prediction and feature
selection. <em>Electronic Journal of Statistics</em>,
<strong>14</strong>(1):2155-2197. DOI: <a href="https://doi.org/10.1214/20-EJS1711">10.1214/20-EJS1711</a>.</p>
<p>Piironen, J. and Vehtari, A. (2017a). Comparison of Bayesian
predictive methods for model selection. <em>Statistics and
Computing</em>, <strong>27</strong>(3):711-735. DOI: <a href="https://doi.org/10.1007/s11222-016-9649-y">10.1007/s11222-016-9649-y</a>.</p>
<p>Piironen, J. and Vehtari, A. (2017b). On the hyperprior choice for
the global shrinkage parameter in the horseshoe prior. In
<em>Proceedings of the 20th International Conference on Artificial
Intelligence and Statistics (AISTATS)</em>, PMLR 54:905-913, 2017. URL:
<a href="https://proceedings.mlr.press/v54/piironen17a.html" class="uri">https://proceedings.mlr.press/v54/piironen17a.html</a>.</p>
<p>Piironen, J. and Vehtari, A. (2017c). Sparsity information and
regularization in the horseshoe and other shrinkage priors.
<em>Electronic Journal of Statistics</em>, <strong>11</strong>(2):
5018-5051. DOI: <a href="https://doi.org/10.1214/17-EJS1337SI">10.1214/17-EJS1337SI</a>.</p>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Currently, the support for additive models is only
experimental in <strong>projpred</strong>.<a href="#fnref1" class="footnote-back">â©ï¸</a></p></li>
<li id="fn2"><p>We will cover custom reference models more deeply in a
future vignette.<a href="#fnref2" class="footnote-back">â©ï¸</a></p></li>
<li id="fn3"><p>More generally, the number of chains is split up as
evenly as possible among the number of CPU cores.<a href="#fnref3" class="footnote-back">â©ï¸</a></p></li>
<li id="fn4"><p>During the forward search, the reference model has
already been projected onto all candidate models (this was where
arguments <code>ndraws</code> and <code>nclusters</code> of
<code>cv_varsel()</code> came into play). During the evaluation of the
submodels along the solution path, the reference model has already been
projected onto those submodels (this was where arguments
<code>ndraws_pred</code> and <code>nclusters_pred</code> of
<code>cv_varsel()</code> came into play). In principle, one could use
the results from the evaluation part for post-selection inference, but
due to a bug in the current implementation (see GitHub issue #168), we
currently have to project once again.<a href="#fnref4" class="footnote-back">â©ï¸</a></p></li>
<li id="fn5"><p><code>proj_linpred()</code> can also transform the
linear predictor to response scale, but here, this is the same as the
linear predictor scale (because of the identity link function).<a href="#fnref5" class="footnote-back">â©ï¸</a></p></li>
<li id="fn6"><p>Beware that this statement is correct here because of
the Gaussian family with the identity link function. For other families
(which usually come in combination with a different link function), one
would typically have to use <code>transform = TRUE</code> in order to
make this statement correct.<a href="#fnref6" class="footnote-back">â©ï¸</a></p></li>
<li id="fn7"><p>In case of the Gaussian family we are using here, the
uncertainty arising from the observational model is the uncertainty due
to the residual standard deviation.<a href="#fnref7" class="footnote-back">â©ï¸</a></p></li>
<li id="fn8"><p>The <code>binomial()</code> family includes the
<code>brms::bernoulli()</code> family as a special case which may only
be used via the <strong>brms</strong> package.<a href="#fnref8" class="footnote-back">â©ï¸</a></p></li>
<li id="fn9"><p>Multilevel models are also known as
<em>hierarchical</em> models or models with <em>partially pooled</em>,
<em>group-level</em>, orâin frequentist termsâ<em>random</em> effects.<a href="#fnref9" class="footnote-back">â©ï¸</a></p></li>
<li id="fn10"><p>Additive terms are also known as <em>smooth</em>
terms.<a href="#fnref10" class="footnote-back">â©ï¸</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
